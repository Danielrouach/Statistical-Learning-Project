# -*- coding: utf-8 -*-
"""Statistical_Learning_Project_2025

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12d72OTUfSPIoZ2eklH-bmCL0cSzWMU0h

# Import
"""

# Core libraries
import numpy as np
import pandas as pd
import re

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Statsmodels
import statsmodels.api as sm
from statsmodels.api import OLS

# Scikit-learn: preprocessing
from sklearn.preprocessing import StandardScaler

# Scikit-learn: model selection
from sklearn.model_selection import KFold, train_test_split
import sklearn.model_selection as skm  # if needed for specific functions

# Scikit-learn: regression models
from sklearn.linear_model import LassoCV, RidgeCV, Ridge, Lasso
from sklearn.linear_model import LinearRegression  # if needed
import sklearn.linear_model as skl  # if needed for generic access

# Scikit-learn: metrics
from sklearn.metrics import mean_squared_error, r2_score

# Scikit-learn: decomposition and PLS
from sklearn.decomposition import PCA
from sklearn.cross_decomposition import PLSRegression

# Scikit-learn: ensemble and tree-based models
from sklearn.ensemble import BaggingRegressor, RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor

# Dataset creation (for synthetic examples)
from sklearn.datasets import make_regression

# Optional external packages (commented if not installed)
# from ISLP.models import Stepwise, sklearn_selected, sklearn_selection_path
# from l0bnb import fit_path

"""# Load data"""

# from google.colab import drive
# drive.mount('/content/drive')

##Load the dataset:
df_partial = pd.read_csv("/content/content_output (1).csv",nrows = 70000,on_bad_lines='skip')
#60083
df_partial.head()

# --- Calculate frames per episode ---
# Group by 'episode_index' and count the number of rows (frames) in each group
frames_per_episode = df_partial.groupby('episode_index').size().reset_index(name='num_frames')

print("\n--- Frames per Episode (Head) ---")
print(frames_per_episode.head())

# --- Plotting the histogram ---
plt.figure(figsize=(10, 6))
sns.histplot(frames_per_episode['num_frames'], bins=20, kde=True) # kde=True adds a density curve
plt.title('Distribution of Number of Frames per Episode', fontsize=16)
plt.xlabel('Number of Frames', fontsize=12)
plt.ylabel('Number of Episodes', fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

print("\n--- Histogram Generated ---")
print(f"Minimum frames per episode: {frames_per_episode['num_frames'].min()}")
print(f"Maximum frames per episode: {frames_per_episode['num_frames'].max()}")
print(f"Average frames per episode: {frames_per_episode['num_frames'].mean():.2f}")
print(f"Median frames per episode: {frames_per_episode['num_frames'].median()}")

df_partial.shape

YY = df_partial.groupby('episode_index')['timestamp'].max().reset_index()

YY['timestamp'].describe()

# keep only first n frames of each episode
frames_to_keep = 30
df_partial = df_partial.groupby('episode_index').head(frames_to_keep).reset_index(drop=True)

df_partial

"""# Preproccessing"""

df_partial['frame_index'] = df_partial['frame_index'].astype(int)
df_partial['episode_index'] = df_partial['episode_index'].astype(int)

"""Convert the action column to features columns"""

# Function to convert string representation to array
def convert_to_array(text):
    if isinstance(text, (list, np.ndarray)):
        return text

    # Remove brackets and split by spaces or commas
    if isinstance(text, str):
        # Clean the string and extract numbers
        text = text.strip('[]')
        # Split by spaces or commas, handling multiple spaces
        values = re.split(r'[\s,]+', text.strip())
        # Convert to float, filtering out empty strings
        values = [float(val) for val in values if val]
        return values

    return text

# Apply conversion to action column
df_partial['action_array'] = df_partial['action'].apply(convert_to_array)

# Check if conversion worked
print("First converted action:", df_partial['action_array'].iloc[0])
print("Type:", type(df_partial['action_array'].iloc[0]))

# Get number of coordinates from first valid row
sample_idx = 0
while sample_idx < len(df_partial) and not isinstance(df_partial['action_array'].iloc[sample_idx], (list, np.ndarray)):
    sample_idx += 1

if sample_idx < len(df_partial):
    num_action_coords = len(df_partial['action_array'].iloc[sample_idx])
    print(f"Number of coordinates: {num_action_coords}")

    # Create new columns for each action coordinate
    for i in range(num_action_coords):
        col_name = f'joint_delta_pos_{i}'
        df_partial[col_name] = df_partial['action_array'].apply(lambda x: x[i] if isinstance(x, (list, np.ndarray)) and i < len(x) else None)

df_partial.head()

# Get all columns that start with 'joint_delta_pos'
delta_pos_columns = [col for col in df_partial.columns if col.startswith('joint_delta_pos')]

# Calculate the norm (square root of sum of squares)
df_partial['joint_delta_pos_norm'] = np.sqrt(
    np.sum(df_partial[delta_pos_columns] ** 2, axis=1)
)

"""Convert the observation column to features columns"""

# Apply the same conversion to observation.state column
df_partial['observation_state_array'] = df_partial['observation.state'].apply(convert_to_array)

# Check if conversion worked
print("First converted observation state:", df_partial['observation_state_array'].iloc[0])
print("Type:", type(df_partial['observation_state_array'].iloc[0]))

# List of column names for observation.state
observation_state_names = [
    "box0_pos_x", "box0_pos_y", "box0_pos_z",
    "box0_pos_rel_eef_x", "box0_pos_rel_eef_y", "box0_pos_rel_eef_z",
    "box0_quat_x", "box0_quat_y", "box0_quat_z", "box0_quat_w",
    "box0_quat_rel_eef_x", "box0_quat_rel_eef_y", "box0_quat_rel_eef_z", "box0_quat_rel_eef_w",
    "box1_pos_x", "box1_pos_y", "box1_pos_z",
    "box1_pos_rel_eef_x", "box1_pos_rel_eef_y", "box1_pos_rel_eef_z",
    "box1_quat_x", "box1_quat_y", "box1_quat_z", "box1_quat_w",
    "box1_quat_rel_eef_x", "box1_quat_rel_eef_y", "box1_quat_rel_eef_z", "box1_quat_rel_eef_w",
    "box2_pos_x", "box2_pos_y", "box2_pos_z",
    "box2_pos_rel_eef_x", "box2_pos_rel_eef_y", "box2_pos_rel_eef_z",
    "box2_quat_x", "box2_quat_y", "box2_quat_z", "box2_quat_w",
    "box2_quat_rel_eef_x", "box2_quat_rel_eef_y", "box2_quat_rel_eef_z", "box2_quat_rel_eef_w",
    "box3_pos_x", "box3_pos_y", "box3_pos_z",
    "box3_pos_rel_eef_x", "box3_pos_rel_eef_y", "box3_pos_rel_eef_z",
    "box3_quat_x", "box3_quat_y", "box3_quat_z", "box3_quat_w",
    "box3_quat_rel_eef_x", "box3_quat_rel_eef_y", "box3_quat_rel_eef_z", "box3_quat_rel_eef_w",
    "eef_pos_x", "eef_pos_y", "eef_pos_z",
    "eef_quat_x", "eef_quat_y", "eef_quat_z", "eef_quat_w",
    "panda_finger_joint1_pos", "panda_finger_joint2_pos",
    "panda_finger_joint1_vel", "panda_finger_joint2_vel",
    "panda_joint1_pos", "panda_joint2_pos", "panda_joint3_pos",
    "panda_joint4_pos", "panda_joint5_pos", "panda_joint6_pos", "panda_joint7_pos",
    "panda_joint1_vel", "panda_joint2_vel", "panda_joint3_vel",
    "panda_joint4_vel", "panda_joint5_vel", "panda_joint6_vel", "panda_joint7_vel"
]

# Get sample row to check length
sample_idx = 0
while sample_idx < len(df_partial) and not isinstance(df_partial['observation_state_array'].iloc[sample_idx], (list, np.ndarray)):
    sample_idx += 1

if sample_idx < len(df_partial):
    array_length = len(df_partial['observation_state_array'].iloc[sample_idx])
    print(f"Length of observation state array: {array_length}")
    print(f"Length of observation state names: {len(observation_state_names)}")

    # Verify if names match array length
    if array_length != len(observation_state_names):
        print("Warning: Array length doesn't match the number of provided names!")
        # Use the minimum to avoid errors
        num_columns = min(array_length, len(observation_state_names))
    else:
        num_columns = array_length

    # Create columns with the specified names
    for i in range(num_columns):
        col_name = observation_state_names[i] if i < len(observation_state_names) else f"obs_state_{i}"
        df_partial[col_name] = df_partial['observation_state_array'].apply(
            lambda x: x[i] if isinstance(x, (list, np.ndarray)) and i < len(x) else None
        )

    # Show first few rows with some of the new columns
    display_cols = ['timestamp', 'episode_index'] + observation_state_names[:81]  # Show first 5 state columns
    print(df_partial[display_cols].head())
else:
    print("No valid observation state array found in the first rows")

"""# Feature Engineering

Create eef feature -> The movement of eef
"""

print("\n--- Calculating Total End-Effector Path Length ---")
shifts = df_partial.groupby('episode_index')[['eef_pos_x', 'eef_pos_y', 'eef_pos_z']].shift(1)

deltas = df_partial[['eef_pos_x', 'eef_pos_y', 'eef_pos_z']] - shifts

df_partial['eef_displacement'] = np.sqrt((deltas**2).sum(axis=1))

df_partial['eef_displacement'] = df_partial['eef_displacement'].fillna(0.0)

print(df_partial['eef_displacement'].head())

"""Create velocity eef feature"""

# another way to do the same:
df_partial['eef_velocity'] = df_partial['eef_displacement'] / (df_partial['timestamp'].iloc[1] - df_partial['timestamp'].iloc[0])

"""# Create Episodes Dataset:"""

df_partial.groupby('episode_index')['timestamp'].max()

import numpy as np
import pandas as pd

def create_episode_features(df_partial, YY):
    """
    Create episode-level features from time-series data.

    Parameters:
    -----------
    df_partial : pd.DataFrame
        Time-series data with episode_index
    YY : pd.DataFrame
        Base episode table to merge features into

    Returns:
    --------
    pd.DataFrame
        Episode table with all engineered features
    """

    # Initialize working dataframes
    df = df_partial.copy()
    episode_table = YY.copy()

    print(f"Starting with {len(episode_table)} episodes and {len(df)} time-series records")

    # =============================================================================
    # 1. EEF DISPLACEMENT FEATURES
    # =============================================================================

    def add_displacement_features(df, episode_table):
        """Add end-effector displacement aggregation features"""
        print("Computing displacement features...")

        # Sum of displacement by episode
        displacement_sum = (df.groupby('episode_index')['eef_displacement']
                          .sum()
                          .reset_index()
                          .rename(columns={"eef_displacement": "displacement_sum"}))

        episode_table = episode_table.merge(displacement_sum, on='episode_index', how='left')

        # Norm of displacement (L2 norm by episode)
        df_temp = df.copy()
        df_temp['eef_displacement_squared'] = df_temp['eef_displacement'] ** 2

        displacement_norm = (df_temp.groupby('episode_index')['eef_displacement_squared']
                           .sum()
                           .reset_index())
        displacement_norm['norm_eef_displacement'] = np.sqrt(displacement_norm['eef_displacement_squared'])
        displacement_norm = displacement_norm[['episode_index', 'norm_eef_displacement']]

        episode_table = episode_table.merge(displacement_norm, on='episode_index', how='left')

        # Comprehensive displacement statistics
        displacement_stats = (df.groupby('episode_index')['eef_displacement']
                            .agg(['mean', 'max', 'sum', 'var', 'std'])
                            .reset_index())

        displacement_stats.columns = ['episode_index', 'mean_eef_displacement', 'max_eef_displacement',
                                    'sum_eef_displacement', 'var_eef_displacement', 'std_eef_displacement']

        episode_table = episode_table.merge(displacement_stats, on='episode_index', how='left')

        return episode_table

    episode_table = add_displacement_features(df, episode_table)

    # =============================================================================
    # 2. EEF VELOCITY FEATURES
    # =============================================================================

    def add_velocity_features(df, episode_table):
        """Add end-effector velocity aggregation features"""
        print("Computing velocity features...")

        velocity_stats = (df.groupby('episode_index')['eef_velocity']
                        .agg(['mean', 'max', 'sum', 'var', 'std'])
                        .reset_index())

        velocity_stats.columns = ['episode_index', 'mean_eef_velocity', 'max_eef_velocity',
                                'sum_eef_velocity', 'var_eef_velocity', 'std_eef_velocity']

        episode_table = episode_table.merge(velocity_stats, on='episode_index', how='left')

        return episode_table

    episode_table = add_velocity_features(df, episode_table)

    # =============================================================================
    # 3. JOINT POSITION DELTA NORMS (Individual joints)
    # =============================================================================

    def add_individual_joint_norms(df, episode_table):
        """Add L2 norm for each joint delta position by episode"""
        print("Computing individual joint delta position norms...")

        df_temp = df.copy()
        joint_norms_list = []

        for joint_idx in range(1, 8):
            col_name = f'joint_delta_pos_{joint_idx}'

            # Square the values
            df_temp[f'{col_name}_squared'] = df_temp[col_name] ** 2

            # Sum squares by episode and take square root
            joint_norm = (df_temp.groupby('episode_index')[f'{col_name}_squared']
                         .sum()
                         .reset_index())
            joint_norm[f'norm_joint_delta_pos{joint_idx}'] = np.sqrt(joint_norm[f'{col_name}_squared'])
            joint_norm = joint_norm[['episode_index', f'norm_joint_delta_pos{joint_idx}']]

            joint_norms_list.append(joint_norm)

        # Merge all joint norms sequentially
        for joint_norm in joint_norms_list:
            episode_table = episode_table.merge(joint_norm, on='episode_index', how='left')

        return episode_table

    episode_table = add_individual_joint_norms(df, episode_table)

    # =============================================================================
    # 4. TOTAL JOINT POSITION DELTA NORM (Sum of per-timestep norms)
    # =============================================================================

    def add_total_joint_norm(df_partial, episode_table):
        """Add total joint delta position norm (sum of per-timestep L2 norms)"""
        print("Computing total joint delta position norm...")

        df_work = df_partial.copy()

        # Get all joint delta position columns
        delta_pos_columns = [col for col in df_work.columns if col.startswith('joint_delta_pos')]

        # Calculate per-timestep L2 norm across all joints
        df_work['joint_delta_pos_norm'] = np.sqrt(
            np.sum(df_work[delta_pos_columns] ** 2, axis=1)
        )

        # Sum these norms by episode
        episode_norm = (df_work.groupby('episode_index')['joint_delta_pos_norm']
                       .sum()
                       .reset_index()
                       .rename(columns={'joint_delta_pos_norm': 'norm_joint_delta_pos_total'}))

        episode_table = episode_table.merge(episode_norm, on='episode_index', how='left')

        return episode_table

    episode_table = add_total_joint_norm(df_partial, episode_table)

    # =============================================================================
    # 5. JOINT 7 SWITCH DETECTION
    # =============================================================================

    def add_joint7_switches(df, episode_table):
        """Count switches in joint_delta_pos_7 within each episode"""
        print("Computing joint 7 switch counts...")

        df_temp = df.copy()

        # Detect value changes
        df_temp['joint_delta_pos_7_switch'] = df_temp['joint_delta_pos_7'].ne(df_temp['joint_delta_pos_7'].shift())

        # Only count switches within the same episode (not across episode boundaries)
        df_temp['switch_flag'] = (df_temp['joint_delta_pos_7_switch'] &
                                 (df_temp['episode_index'] == df_temp['episode_index'].shift()))

        # Count switches per episode
        switch_counts = (df_temp.groupby('episode_index')['switch_flag']
                        .sum()
                        .reset_index()
                        .rename(columns={'switch_flag': 'joint_delta_pos_7_switches'}))

        episode_table = episode_table.merge(switch_counts, on='episode_index', how='left')

        return episode_table

    episode_table = add_joint7_switches(df, episode_table)

    # =============================================================================
    # 6. BOX-EEF SPATIAL FEATURES
    # =============================================================================

    def add_box_eef_features(df_partial, episode_table):
        """Add box-to-end-effector distance and quaternion angle features"""
        print("Computing box-EEF spatial features...")

        df_work = df_partial.copy()

        # Euclidean distances from each box to end-effector
        for box_idx in range(4):
            df_work[f"box{box_idx}_eef_dist"] = np.sqrt(
                df_work[f"box{box_idx}_pos_rel_eef_x"]**2 +
                df_work[f"box{box_idx}_pos_rel_eef_y"]**2 +
                df_work[f"box{box_idx}_pos_rel_eef_z"]**2
            )

        # Quaternion angle differences
        def quat_angle_diff(q1, q2):
            """Returns angular difference between two unit quaternions"""
            dot = np.clip(np.sum(q1 * q2, axis=1), -1.0, 1.0)
            return 2 * np.arccos(np.abs(dot))

        # Box-to-EEF quaternion angles
        for box_idx in range(4):
            q_box = df_work[[f"box{box_idx}_quat_x", f"box{box_idx}_quat_y",
                           f"box{box_idx}_quat_z", f"box{box_idx}_quat_w"]].values
            q_eef = df_work[["eef_quat_x", "eef_quat_y", "eef_quat_z", "eef_quat_w"]].values
            df_work[f"box{box_idx}_eef_quat_angle"] = quat_angle_diff(q_box, q_eef)

        # Box-to-Box quaternion dot products
        for i in range(4):
            for j in range(i + 1, 4):
                box_i_quat = df_work[[f"box{i}_quat_x", f"box{i}_quat_y",
                                    f"box{i}_quat_z", f"box{i}_quat_w"]].values
                box_j_quat = df_work[[f"box{j}_quat_x", f"box{j}_quat_y",
                                    f"box{j}_quat_z", f"box{j}_quat_w"]].values
                df_work[f"box{i}_box{j}_quat_dot"] = np.sum(box_i_quat * box_j_quat, axis=1)

        # Box-to-Box Euclidean distances
        for i in range(4):
            for j in range(i + 1, 4):
                # Calculate Euclidean distance between box i and box j positions
                df_work[f"box{i}_box{j}_dist"] = np.sqrt(
                    (df_work[f"box{i}_pos_x"] - df_work[f"box{j}_pos_x"])**2 +
                    (df_work[f"box{i}_pos_y"] - df_work[f"box{j}_pos_y"])**2 +
                    (df_work[f"box{i}_pos_z"] - df_work[f"box{j}_pos_z"])**2
                )

        return df_work, episode_table

    df_with_spatial, episode_table = add_box_eef_features(df_partial, episode_table)

    # =============================================================================
    # 7. AGGREGATE SPATIAL FEATURES BY EPISODE
    # =============================================================================

    def add_spatial_aggregates(df_with_spatial, episode_table):
        """Aggregate spatial features (distances, angles, dots) by episode"""
        print("Aggregating spatial features by episode...")

        # Select spatial feature columns
        spatial_features = [col for col in df_with_spatial.columns if (
            ("box" in col and "dist" in col) or
            ("quat_angle" in col) or
            ("quat_dot" in col)
        )]

        print(f"Found {len(spatial_features)} spatial features to aggregate")

        # Aggregate with mean, min, max
        spatial_agg = (df_with_spatial.groupby("episode_index")[spatial_features]
                      .agg(['mean', 'min', 'max']))

        # Flatten MultiIndex columns
        spatial_agg.columns = [f"{col}_{stat}" for col, stat in spatial_agg.columns]
        spatial_agg = spatial_agg.reset_index()

        # Merge with episode table
        episode_table = episode_table.merge(spatial_agg, on="episode_index", how="left")

        return episode_table

    episode_table = add_spatial_aggregates(df_with_spatial, episode_table)

    # =============================================================================
    # 8. HANDLE TIMESTAMP COLUMN RENAME
    # =============================================================================

    if "timestamp" in episode_table.columns:
        episode_table.rename(columns={"timestamp": "max_of_timestamp"}, inplace=True)
        print("Renamed 'timestamp' column to 'max_of_timestamp'")

    # =============================================================================
    # 9. FINAL VALIDATION AND SUMMARY
    # =============================================================================

    print(f"\nFeature engineering complete!")
    print(f"Final episode table shape: {episode_table.shape}")
    print(f"Total features created: {len(episode_table.columns) - len(YY.columns)}")

    # Check for any missing values in key features
    key_features = ['displacement_sum', 'norm_eef_displacement', 'norm_joint_delta_pos_total']
    for feature in key_features:
        if feature in episode_table.columns:
            missing_count = episode_table[feature].isna().sum()
            if missing_count > 0:
                print(f"WARNING: {missing_count} missing values in {feature}")

    return episode_table

# =============================================================================
# MAIN EXECUTION
# =============================================================================

# Usage:
episode_table = create_episode_features(df_partial, YY)

# Redefine the extended feature engineering pipeline after reset
def add_joint_total_angular_displacement(df_partial, episode_table):
    """Add total angular displacement per joint across time within each episode."""
    print("\n--- Calculating Total Angular Displacement Per Joint ---")

    df_sorted = df_partial.sort_values(by=['episode_index', 'timestamp'])

    joint_pos_cols = [f'panda_joint{i}_pos' for i in range(1, 8)]
    joint_displacements = df_sorted.groupby('episode_index')[joint_pos_cols].apply(lambda group: group.diff().abs().sum()).reset_index()
    joint_displacements.rename(columns={f'panda_joint{i}_pos': f'joint{i}_total_displacement' for i in range(1, 8)}, inplace=True)

    return episode_table.merge(joint_displacements, on='episode_index', how='left')



def add_gripper_action_changes(df_partial, episode_table):
    """Count number of gripper action changes based on joint_delta_pos_7."""
    print("\n--- Calculating Gripper Action Change Count ---")

    df_sorted = df_partial.sort_values(by=['episode_index', 'timestamp'])
    gripper_changes = df_sorted.groupby('episode_index').apply(
        lambda group: (group['joint_delta_pos_7'].diff().iloc[1:] != 0).sum()
    ).reset_index(name='gripper_action_changes_count')

    return episode_table.merge(gripper_changes, on='episode_index', how='left')



episode_table = add_joint_total_angular_displacement(df_partial, episode_table)
episode_table = add_gripper_action_changes(df_partial, episode_table)

episode_table.shape

plt.hist(episode_table["max_of_timestamp"],bins = 30)
plt.show()
plt.title('log scaled episodes duration')
plt.hist(np.log(episode_table["max_of_timestamp"]),bins = 30)

import seaborn as sns
import matplotlib.pyplot as plt

# Select only the delta_pos columns
delta_pos_cols = ['max_of_timestamp'] + [f'box{i}_eef_dist_mean' for i in range(0, 4)]
delta_pos_data = episode_table[delta_pos_cols]

# Compute the correlation matrix
corr_matrix = delta_pos_data.corr()

# Plot the correlation matrix using a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(
    corr_matrix,
    annot=True,      # Show correlation values
    fmt=".2f",       # Format numbers to 2 decimal places
    cmap='coolwarm', # Color map
    square=True,
    linewidths=0.5,
    cbar_kws={"shrink": 0.8}
)

plt.title('Correlation Matrix of Panda Joint delta_pos', fontsize=16)
plt.xticks(rotation=45)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Select only the delta_pos columns
delta_pos_cols = ['max_of_timestamp'] + [f'box0_box{i}_quat_dot_mean' for i in range(1, 4)]
delta_pos_data = episode_table[delta_pos_cols]

# Compute the correlation matrix
corr_matrix = delta_pos_data.corr()

# Plot the correlation matrix using a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(
    corr_matrix,
    annot=True,      # Show correlation values
    fmt=".2f",       # Format numbers to 2 decimal places
    cmap='coolwarm', # Color map
    square=True,
    linewidths=0.5,
    cbar_kws={"shrink": 0.8}
)

plt.title('Correlation Matrix of Panda Joint delta_pos', fontsize=16)
plt.xticks(rotation=45)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Select only the delta_pos columns
delta_pos_cols = ['max_of_timestamp'] + [f'box0_box{i}_dist_mean' for i in range(1, 4)]
delta_pos_data = episode_table[delta_pos_cols]

# Compute the correlation matrix
corr_matrix = delta_pos_data.corr()

# Plot the correlation matrix using a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(
    corr_matrix,
    annot=True,      # Show correlation values
    fmt=".2f",       # Format numbers to 2 decimal places
    cmap='coolwarm', # Color map
    square=True,
    linewidths=0.5,
    cbar_kws={"shrink": 0.8}
)

plt.title('Correlation Matrix of Panda Joint delta_pos', fontsize=16)
plt.xticks(rotation=45)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

corr_threshold = 0.9
# Drop correlated features
feature_cols = [col for col in episode_table.columns if col != "episode_index"]
corr_matrix = episode_table[feature_cols].corr().abs()

# Select upper triangle of correlation matrix
upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

# Find features with correlation greater than threshold
to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > corr_threshold)]

# Drop from episode table
episode_table = episode_table.drop(columns=to_drop)

print(f"Removed {len(to_drop)} highly correlated features:")
print(sorted(to_drop))

def find_highly_correlated_features(df, threshold=0.9):
    """
    Identify pairs of features with correlation above a given threshold.

    Parameters:
    -----------
    df : pd.DataFrame
        DataFrame containing only numerical features.
    threshold : float
        Minimum absolute correlation to be considered high.

    Returns:
    --------
    List of tuples: [(feature1, feature2, correlation), ...]
    """
    # Compute correlation matrix
    corr_matrix = df.corr().abs()

    # Create boolean mask for upper triangle
    upper = np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)

    # Apply mask
    upper_corr = corr_matrix.where(upper)

    # Extract highly correlated pairs
    high_corr = [
        (col1, col2, upper_corr.loc[col1, col2])
        for col1 in upper_corr.columns
        for col2 in upper_corr.columns
        if col1 != col2 and upper_corr.loc[col1, col2] > threshold
    ]

    # Sort by correlation value
    return sorted(high_corr, key=lambda x: -x[2])

# Example usage:
# Assume X is your features dataframe (exclude target column)
high_corr_features = find_highly_correlated_features(episode_table, threshold=0.9)
for f1, f2, corr in high_corr_features:
    print(f"{f1} and {f2} have correlation {corr:.3f}")

import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import linregress

# Define the target variable (episode duration)
target_col = 'max_of_timestamp'

# Get all numeric feature columns except the target
feature_cols = [col for col in episode_table.columns if col != target_col and episode_table[col].dtype in ['float64', 'int64']]

# Set up the figure with subplots
num_features = len(feature_cols)
num_rows = (num_features // 4) + (1 if num_features % 4 != 0 else 0)  # Determine rows dynamically
fig, axes = plt.subplots(num_rows, 4, figsize=(20, 5 * num_rows))
axes = axes.flatten()  # Flatten axes for easy indexing

# Create scatter plot for each feature vs episode duration
for i, feature_col in enumerate(feature_cols):
    x = episode_table[feature_col]
    y = episode_table[target_col]

    # Calculate regression
    slope, intercept, r_value, p_value, std_err = linregress(x, y)
    equation = f"y = {slope:.2f}x + {intercept:.2f}"
    r_squared = f"$R^2$ = {r_value**2:.2f}"

    # Plot on the appropriate subplot
    ax = axes[i]
    sns.scatterplot(x=x, y=y, s=80, alpha=0.7, ax=ax)
    sns.regplot(x=x, y=y, scatter=False, color='red', ax=ax)

    # Add title and labels
    ax.set_title(f'Episode Duration vs {feature_col}', fontsize=12)
    ax.set_ylabel('Episode Duration (sum_of_timestamp)', fontsize=10)
    ax.set_xlabel(f'{feature_col}', fontsize=10)
    ax.grid(True, linestyle='--', alpha=0.7)

    # Add annotation with equation and R^2
    ax.text(0.05, 0.95, f"{equation}\n{r_squared}",
            transform=ax.transAxes,
            fontsize=10,
            verticalalignment='top',
            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.5))

# Remove unused subplots if there are extra
for j in range(i+1, len(axes)):
    fig.delaxes(axes[j])

# Adjust layout and add main title
plt.tight_layout()
fig.suptitle('Relationship Between Episode Duration and Features', fontsize=16, y=1.02)

plt.show()

episode_table.describe()

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.hist(episode_table["max_of_timestamp"], bins=30, color='skyblue', edgecolor='black')
plt.title("Distribution of Episode Durations", fontsize=14)
plt.xlabel("Total Duration per Episode (seconds)", fontsize=12)
plt.ylabel("Number of Episodes", fontsize=12)
plt.grid(True, axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
plt.hist(episode_table["max_of_timestamp"], bins=30, color='skyblue', edgecolor='black')
plt.yscale('log')  # Set y-axis to log scale
plt.xlabel("Distribution of Episode Durations")
plt.ylabel("Frequency (Log Scale)")
plt.title("Log-Scaled Histogram of Episode Durations")
plt.grid(True, which="both", linestyle='--', linewidth=0.5)
plt.tight_layout()
plt.show()

"""# Modeling"""

# based on analysis above
columns_to_drop = ['episode_index', 'box0_box1_quat_dot_mean',
       'box0_box1_quat_dot_min', 'box0_box1_quat_dot_max',
       'box0_box2_quat_dot_mean', 'box0_box2_quat_dot_min',
       'box0_box2_quat_dot_max', 'box0_box3_quat_dot_mean',
       'box0_box3_quat_dot_min', 'box0_box3_quat_dot_max',
       'box1_box2_quat_dot_mean', 'box1_box2_quat_dot_min',
       'box1_box2_quat_dot_max', 'box1_box3_quat_dot_mean',
       'box1_box3_quat_dot_min', 'box1_box3_quat_dot_max',
       'box2_box3_quat_dot_mean', 'box2_box3_quat_dot_min',
       'box2_box3_quat_dot_max', 'norm_joint_delta_pos7']

"""## normalization"""

episode_table['logged_max_of_timestamp'] = np.log(episode_table['max_of_timestamp'])

episode_table.columns

df = episode_table.copy()

# Drop non-feature columns if necessary
columns_to_drop = [col for col in columns_to_drop if col in df.columns]
df = df.drop(columns=columns_to_drop)  # Drop if not needed for modeling

excluded_column = ['max_of_timestamp', 'logged_max_of_timestamp']

# Create a copy of the DataFrame to store normalized data
episode_table_normalized = episode_table.copy()

# List of columns to normalize (excluding the specific column)
columns_to_normalize = [col for col in df.columns if col not in excluded_column]

# Initialize the scaler
scaler = StandardScaler()

# Fit and transform only the columns to normalize
episode_table_normalized[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])

df = episode_table_normalized.copy()

# Drop non-feature columns if necessary
df = df.drop(columns=columns_to_drop)  # Drop if not needed for modeling

# Define target and predictors
y = df["logged_max_of_timestamp"]
# y = df["max_of_timestamp"]
X = df.drop(columns=excluded_column)

X.shape

# also drop zero corr columns

import numpy as np
import pandas as pd

# Compute Pearson correlation for each feature with target
correlations = X.apply(lambda col: np.corrcoef(col, y.squeeze())[0, 1])

# Replace NaNs with 0 (in case of constant columns)
correlations = correlations.fillna(0)

# Define threshold: here strictly zero correlation
mask = np.abs(correlations) > 0.05

# Select only features with non-zero correlation
X_filtered = X.loc[:, mask]

# Optional: print removed columns
removed = X.columns[~mask]
print("Removed features with zero correlation:", list(removed))

X = X_filtered

X.shape

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""## Forward Stepwise Regression"""

import statsmodels.api as sm
import numpy as np
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error

def cross_val_mse(model_func, X, y, predictors, n_splits=5, random_state=42):
    """
    Computes the average cross-validation MSE for a given set of predictors.
    """
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)
    mse_list = []

    for train_idx, val_idx in kf.split(X):
        X_train_cv, X_val_cv = X.iloc[train_idx], X.iloc[val_idx]
        y_train_cv, y_val_cv = y.iloc[train_idx], y.iloc[val_idx]

        # Fit model
        X_train_const = sm.add_constant(X_train_cv[predictors])
        model = sm.OLS(y_train_cv, X_train_const).fit()

        # Predict on validation fold
        X_val_const = sm.add_constant(X_val_cv[predictors], has_constant='add')
        y_pred = model.predict(X_val_const)

        mse = mean_squared_error(y_val_cv, y_pred)
        mse_list.append(mse)

    return np.mean(mse_list)

def forward_stepwise_selection_cv(X, y, verbose=True, n_splits=5):
    predictors = []
    remaining = set(X.columns)
    best_models = []

    for k in range(len(X.columns)):
        best_mse = np.inf
        best_model = None
        best_predictor = None

        for predictor in remaining:
            trial_predictors = predictors + [predictor]
            # Compute cross-validation MSE
            mse = cross_val_mse(sm.OLS, X, y, trial_predictors, n_splits=n_splits)

            if mse < best_mse:
                best_mse = mse
                # Train the final model with all data for the selected predictors
                X_full = sm.add_constant(X[trial_predictors])
                model = sm.OLS(y, X_full).fit()
                best_model = model
                best_predictor = predictor

        if best_models and best_mse > best_models[-1][2] * 3:
          print(f"Stop because the error is raising too much")
          break

        if best_predictor is not None:
            predictors.append(best_predictor)
            remaining.remove(best_predictor)
            best_models.append((best_model, predictors.copy(), best_mse))

            if verbose:
                print(f"Step {k+1}: Add '{best_predictor}' | Cross-Validated MSE = {best_mse:.4f}")

    return best_models

# Usage:
best_models = forward_stepwise_selection_cv(X_train, y_train, n_splits=5)

#Select the model with the lowest CV MSE
final_model, final_features, final_mse = min(best_models, key=lambda x: x[2])

print("\n✅ Final Model Features Based on Cross-Validated MSE:")
print(final_features)
print(f"Final CV MSE: {final_mse:.4f}")
final_model.summary()

# Visualize the results
# Extract MSE values and number of predictors
mse_values = [m[2] for m in best_models]
num_predictors = [len(m[1]) for m in best_models]

# # Plot Test MSE vs. Number of Predictors
# plt.figure(figsize=(8, 5))
# plt.plot(num_predictors, mse_values, 'go-', label="Cross-Validated MSE")
# plt.xlabel("Number of Predictors")
# plt.ylabel("Cross-Validated Mean Squared Error")
# plt.title("Forward Stepwise Selection: Cross-Validated MSE")
# plt.grid(True)
# plt.tight_layout()
# plt.show()

# Plot with best model highlighted
best_idx = mse_values.index(min(mse_values))
best_num_predictors = num_predictors[best_idx]
best_mse = mse_values[best_idx]

plt.figure(figsize=(8, 5))
plt.plot(num_predictors, mse_values, 'go-', label="Cross-Validated MSE")
plt.plot(best_num_predictors, best_mse, 'ro', markersize=8,
         label=f"Best model: {best_num_predictors} predictors, MSE={best_mse:.4f}")
plt.xlabel("Number of Predictors")
plt.ylabel("Cross-Validated Mean Squared Error")
plt.title("Forward Stepwise Selection: Cross-Validated MSE with Best Model Highlighted")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

final_r_squared = final_model.rsquared
print(f"The R_squared of the model is: {final_r_squared:.4f}")

# Column Plot of Beta Coefficients for the Final Model
final_betas = final_model.params.drop('const', errors='ignore') # Drop the intercept
if not final_betas.empty:
    plt.figure(figsize=(10, 6))
    final_betas.plot(kind='bar')
    plt.title("Beta Coefficients for the Final Model")
    plt.xlabel("Feature Name")
    plt.ylabel("Coefficient Value")
    plt.grid(axis='y')
    plt.tight_layout()
    plt.show()
else:
    print("\n⚠️ No features selected for the final model, so no beta coefficients to plot.")

print(final_betas)

"""## Lasso Regression"""

def lasso_cross_validation(X, y, alphas=None, n_folds=5, random_state=42):
    """
    Performs Lasso regression with cross-validation to select the optimal alpha (lambda).

    Args:
        X (pd.DataFrame): Feature matrix.
        y (pd.Series): Target variable.
        alphas (np.ndarray, optional): Array of alpha values to try. If None, LassoCV will use its own.
        n_folds (int): Number of cross-validation folds.
        random_state (int): Random seed for reproducibility.

    Returns:
        tuple: (best_alpha, mse_values, alphas_used, fitted_lasso_cv)
               - best_alpha (float): The optimal alpha value found by cross-validation.
               - mse_values (np.ndarray): Array of mean squared errors for each alpha value.
               - alphas_used (np.ndarray): The actual alpha values used during cross-validation.
               - fitted_lasso_cv (LassoCV): The fitted LassoCV object.
    """

    lasso_cv = LassoCV(alphas=alphas, cv=n_folds, random_state=random_state, n_jobs=-1)
    lasso_cv.fit(X, y)

    return lasso_cv.alpha_, lasso_cv.mse_path_.mean(axis=-1), lasso_cv.alphas_, lasso_cv

# Usage:
# Assuming X and y are already defined as pandas DataFrame and Series respectively

# Define a range of alpha values to try (optional)
alphas_to_try = np.logspace(-4, 2, 200)

best_alpha, mse_values, alphas_used, lasso_cv_model = lasso_cross_validation(X_train, y_train, alphas=alphas_to_try, n_folds=5, random_state=42)

print("\n✅ Best Alpha (Lambda) found by Cross-Validation:", best_alpha)
print("\n✅ MSE Values for different Alphas:\n", pd.Series(mse_values, index=alphas_used))

# Get the best MSE for plotting
best_mse = np.min(mse_values)

# Plot MSE as a function of Lambda (Alpha)
plt.figure(figsize=(10, 6))
plt.plot(alphas_used, mse_values, marker='.', linestyle='-', label='MSE Curve')
plt.scatter(best_alpha, best_mse, color='red', zorder=5, label=f'Best λ = {best_alpha:.5f}')
plt.axvline(best_alpha, color='red', linestyle='--', linewidth=1)

plt.xscale('log')
plt.xlabel('Lambda (Alpha)')
plt.ylabel('Mean Squared Error (Cross-Validated)')
plt.title('Lasso Regression: MSE vs. Lambda')
plt.grid(True)
plt.gca().invert_xaxis()  # Optional: Invert x-axis to show larger alpha on the left
plt.legend()
plt.tight_layout()
plt.show()

best_coefficients = pd.Series(lasso_cv_model.coef_, index=X.columns)
print("\n✅ Coefficients of the Best Lasso Model:")
print(best_coefficients)

final_r_squared = lasso_cv_model.score(X_train, y_train)
print(f"The R squared of the model is: {final_r_squared:.4f}")

def estimate_bias_variance(X, y, alphas, n_splits=5, random_state=42):
    """
    Estimates the bias and variance for different alpha values in Lasso regression
    using cross-validation.

    Args:ac
        X (pd.DataFrame): Feature matrix.
        y (pd.Series): Target variable.
        alphas (np.ndarray): Array of alpha values to try.
        n_splits (int): Number of cross-validation folds.
        random_state (int): Random seed for reproducibility.

    Returns:
        tuple: (alphas, bias_squared_values, variance_values)
               - alphas (np.ndarray): The alpha values used.
               - bias_squared_values (np.ndarray): Estimated squared bias for each alpha.
               - variance_values (np.ndarray): Estimated variance for each alpha.
    """
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)

    bias_squared_values = []
    variance_values = []

    # We'll use a single train-test split to estimate the "true" model performance
    X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.3, random_state=random_state)

    for alpha in alphas:
        predictions = []
        models = []
        kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)

        for train_idx, val_idx in kf.split(X_train):
            X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]
            y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]

            lasso = Lasso(alpha=alpha, random_state=random_state)
            lasso.fit(X_train_fold, y_train_fold)
            models.append(lasso)
            predictions.append(lasso.predict(X_test))

        # Estimate Bias^2 (squared difference between average prediction and true value)
        avg_prediction = np.mean(predictions, axis=0)
        bias_squared = np.mean((avg_prediction - y_test)**2)
        bias_squared_values.append(bias_squared)

        # Estimate Variance (average squared difference of predictions around their mean)
        variance = np.mean(np.var(predictions, axis=0))
        variance_values.append(variance)

    return alphas, np.array(bias_squared_values), np.array(variance_values)

# Usage:
# Assuming X and y are already defined

# Define a range of alpha values to try
alphas_to_try = np.logspace(-4, 2, 200)

# Estimate bias and variance
alphas_plot, bias_sq, variance = estimate_bias_variance(X, y, alphas_to_try, n_splits=5, random_state=42)

# Plot Bias^2 and Variance as a function of Lambda (Alpha)
plt.figure(figsize=(12, 6))
plt.plot(alphas_plot, bias_sq, label='Squared Bias', linestyle='--')
plt.plot(alphas_plot, variance, label='Variance', linestyle='-')
plt.xscale('log')
plt.xlabel('Lambda (Alpha)')
plt.ylabel('Bias^2 / Variance')
plt.title('Lasso Regression: Bias-Variance Trade-off vs. Lambda')
plt.legend()
plt.grid(True)
plt.gca().invert_xaxis() # Optional: Invert x-axis
plt.tight_layout()


# Combine and plot Bias^2 + Variance (approximation of MSE)
plt.figure(figsize=(12, 6))
plt.plot(alphas_plot, bias_sq + variance, label='Bias^2 + Variance (Approx. MSE)', linestyle='-.')
plt.xscale('log')
plt.xlabel('Lambda (Alpha)')
plt.ylabel('Bias^2 + Variance')
plt.title('Lasso Regression: Bias-Variance + MSE Approximation vs. Lambda')
plt.legend()
plt.grid(True)
plt.gca().invert_xaxis() # Optional: Invert x-axis
plt.tight_layout()
plt.show()

"""## Ridge Regression"""

from sklearn.linear_model import Ridge



def ridge_cross_validation(X, y, alphas=None, n_folds=5, scoring='neg_mean_squared_error'):
    """
    Performs Ridge regression with cross-validation to select the optimal alpha (lambda).

    Args:
        X (pd.DataFrame): Feature matrix.
        y (pd.Series): Target variable.
        alphas (np.ndarray, optional): Array of alpha values to try.
        n_folds (int): Number of cross-validation folds.
        scoring (str): Scoring metric for cross-validation.

    Returns:
        tuple: (best_alpha, mse_values, alphas_used, fitted_ridge_cv)
               - best_alpha (float): The optimal alpha value found by cross-validation.
               - mse_values (np.ndarray): Mean squared error for each alpha.
               - alphas (np.ndarray): The alpha values tested.
               - fitted_ridge_cv (RidgeCV): The fitted RidgeCV object.
    """

    ridge_cv = RidgeCV(alphas=alphas, cv=n_folds, scoring=scoring)
    ridge_cv.fit(X, y)

    # Note: RidgeCV does not expose MSE path directly; we'll compute manually
    mse_values = []
    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)
    for alpha in alphas:
        model = sm.OLS if hasattr(sm, 'OLS') else Ridge(alpha=alpha)
        fold_mse = []
        for train_idx, val_idx in kf.split(X):
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

            ridge = RidgeCV(alphas=[alpha])
            ridge.fit(X_train, y_train)
            y_pred = ridge.predict(X_val)
            fold_mse.append(mean_squared_error(y_val, y_pred))
        mse_values.append(np.mean(fold_mse))

    return ridge_cv.alpha_, np.array(mse_values), alphas, ridge_cv

alphas_to_try = np.logspace(-4, 2, 200)

best_alpha, mse_values, alphas_used, ridge_cv_model = ridge_cross_validation(X_train, y_train, alphas=alphas_to_try, n_folds=5)

print("\n✅ Best Alpha (Lambda) found by Cross-Validation:", best_alpha)
print("\n✅ MSE Values for different Alphas:\n", pd.Series(mse_values, index=alphas_used))

# Get the best MSE value for highlighting
best_mse = np.min(mse_values)

# Plot MSE vs. Alpha
plt.figure(figsize=(10, 6))
plt.plot(alphas_used, mse_values, marker='.', linestyle='-', label='MSE Curve')

# Highlight the best alpha
plt.scatter(best_alpha, best_mse, color='red', zorder=5, label=f'Best λ = {best_alpha:.5f}')
plt.axvline(best_alpha, color='red', linestyle='--', linewidth=1)

# Plot formatting
plt.xscale('log')
plt.xlabel('Lambda (Alpha)')
plt.ylabel('Mean Squared Error (Cross-Validated)')
plt.title('Ridge Regression: MSE vs. Lambda')
plt.grid(True)
plt.gca().invert_xaxis()
plt.legend()
plt.tight_layout()
plt.show()

# Best coefficients
best_coefficients = pd.Series(ridge_cv_model.coef_, index=X.columns)
print("\n✅ Coefficients of the Best Ridge Model:")
print(best_coefficients)

# Plot coefficients
plt.figure(figsize=(10, 6))
best_coefficients.plot(kind='bar')
plt.title('Ridge Coefficients for Best Alpha')
plt.xlabel('Feature')
plt.ylabel('Coefficient Value')
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# Determine the R-squared value of the best Ridge model
final_r_squared = ridge_cv_model.score(X, y)

print(f"The  R-squared of the final model is: {final_r_squared:.4f}")

"""##Random Forest + Bagging"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import BaggingRegressor, RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import time

def compare_ensemble_methods(X, y, max_trees=200, step_size=20, test_size=0.2, random_state=42):
    """
    Compare different ensemble methods using OOB scores.

    Parameters:
    -----------
    X : array-like, shape (n_samples, n_features)
        Feature matrix
    y : array-like, shape (n_samples,)
        Target values
    max_trees : int, default=200
        Maximum number of trees to test
    step_size : int, default=20
        Step size for number of trees
    test_size : float, default=0.2
        Proportion of data to hold out for final validation
    random_state : int, default=42
        Random state for reproducibility

    Returns:
    --------
    dict : Dictionary containing results and fitted models
    """

    print(f"Dataset shape: {X.shape}")
    print(f"Features: {X.shape[1]}, Samples: {X.shape[0]}")

    # Split data - keep larger portion for training to get better OOB estimates
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )

    print(f"Training set: {X_train.shape[0]} samples")
    print(f"Test set: {X_test.shape[0]} samples")

    # Set range of number of trees (optimized for your dataset size)
    n_trees_list = np.arange(10, max_trees + 1, step_size)
    print(f"Testing {len(n_trees_list)} different tree counts: {n_trees_list[0]} to {n_trees_list[-1]}")

    # Initialize results storage
    results = {
        'n_trees': n_trees_list,
        'oob_mse_bagging': [],
        'oob_mse_rf_full': [],
        'oob_mse_rf_half': [],
        'oob_mse_rf_sqrt': [],
        'oob_mse_rf_log2': [],  # Added log2(p) option
        'training_times': []
    }

    # Calculate variance of y_train for converting OOB R^2 to MSE
    y_train_variance = np.var(y_train)
    print(f"Target variance: {y_train_variance:.4f}")

    # Feature counts for different Random Forest configurations
    n_features = X_train.shape[1]
    features_full = n_features
    features_half = max(1, n_features // 2)
    features_sqrt = max(1, int(np.sqrt(n_features)))
    features_log2 = max(1, int(np.log2(n_features)))

    print(f"Feature subsampling - Full: {features_full}, Half: {features_half}, "
          f"Sqrt: {features_sqrt}, Log2: {features_log2}")

    # Iterate over number of trees
    for i, n_trees in enumerate(n_trees_list):
        print(f"\nProgress: {i+1}/{len(n_trees_list)} - Testing {n_trees} trees...")
        iteration_start = time.time()

        # 1. Bagging (full features, no feature subsampling)
        try:
            bag = BaggingRegressor(
                estimator=DecisionTreeRegressor(max_depth=10, min_samples_split=5),  # Limit depth for speed
                n_estimators=n_trees,
                max_features=1.0,  # Use all features
                bootstrap=True,
                oob_score=True,
                random_state=random_state,
                n_jobs=-1
            )
            bag.fit(X_train, y_train)

            if not np.isnan(bag.oob_score_) and bag.oob_score_ <= 1:
                oob_mse = y_train_variance * (1 - bag.oob_score_)
                results['oob_mse_bagging'].append(oob_mse)
            else:
                results['oob_mse_bagging'].append(np.nan)
        except Exception as e:
            print(f"Bagging failed: {e}")
            results['oob_mse_bagging'].append(np.nan)

        # 2. Random Forest with all features
        try:
            rf_full = RandomForestRegressor(
                n_estimators=n_trees,
                max_features=features_full,
                max_depth=10,  # Limit depth for speed
                min_samples_split=5,
                bootstrap=True,
                oob_score=True,
                random_state=random_state,
                n_jobs=-1
            )
            rf_full.fit(X_train, y_train)

            if not np.isnan(rf_full.oob_score_) and rf_full.oob_score_ <= 1:
                oob_mse = y_train_variance * (1 - rf_full.oob_score_)
                results['oob_mse_rf_full'].append(oob_mse)
            else:
                results['oob_mse_rf_full'].append(np.nan)
        except Exception as e:
            print(f"RF Full failed: {e}")
            results['oob_mse_rf_full'].append(np.nan)

        # 3. Random Forest with half features
        try:
            rf_half = RandomForestRegressor(
                n_estimators=n_trees,
                max_features=features_half,
                max_depth=10,
                min_samples_split=5,
                bootstrap=True,
                oob_score=True,
                random_state=random_state,
                n_jobs=-1
            )
            rf_half.fit(X_train, y_train)

            if not np.isnan(rf_half.oob_score_) and rf_half.oob_score_ <= 1:
                oob_mse = y_train_variance * (1 - rf_half.oob_score_)
                results['oob_mse_rf_half'].append(oob_mse)
            else:
                results['oob_mse_rf_half'].append(np.nan)
        except Exception as e:
            print(f"RF Half failed: {e}")
            results['oob_mse_rf_half'].append(np.nan)

        # 4. Random Forest with sqrt(p) features
        try:
            rf_sqrt = RandomForestRegressor(
                n_estimators=n_trees,
                max_features='sqrt',  # sklearn handles this automatically
                max_depth=10,
                min_samples_split=5,
                bootstrap=True,
                oob_score=True,
                random_state=random_state,
                n_jobs=-1
            )
            rf_sqrt.fit(X_train, y_train)

            if not np.isnan(rf_sqrt.oob_score_) and rf_sqrt.oob_score_ <= 1:
                oob_mse = y_train_variance * (1 - rf_sqrt.oob_score_)
                results['oob_mse_rf_sqrt'].append(oob_mse)
            else:
                results['oob_mse_rf_sqrt'].append(np.nan)
        except Exception as e:
            print(f"RF Sqrt failed: {e}")
            results['oob_mse_rf_sqrt'].append(np.nan)

        # 5. Random Forest with log2(p) features
        try:
            rf_log2 = RandomForestRegressor(
                n_estimators=n_trees,
                max_features='log2',  # sklearn handles this automatically
                max_depth=10,
                min_samples_split=5,
                bootstrap=True,
                oob_score=True,
                random_state=random_state,
                n_jobs=-1
            )
            rf_log2.fit(X_train, y_train)

            if not np.isnan(rf_log2.oob_score_) and rf_log2.oob_score_ <= 1:
                oob_mse = y_train_variance * (1 - rf_log2.oob_score_)
                results['oob_mse_rf_log2'].append(oob_mse)
            else:
                results['oob_mse_rf_log2'].append(np.nan)
        except Exception as e:
            print(f"RF Log2 failed: {e}")
            results['oob_mse_rf_log2'].append(np.nan)


        iteration_time = time.time() - iteration_start
        results['training_times'].append(iteration_time)
        print(f"Iteration completed in {iteration_time:.2f}s")

    # Store test data for final validation if needed
    results['X_test'] = X_test
    results['y_test'] = y_test
    results['X_train'] = X_train
    results['y_train'] = y_train

    return results

def plot_ensemble_comparison(results, figsize=(12, 8)):
    """Plot the ensemble comparison results"""

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)

    # Main OOB MSE plot
    n_trees_list = results['n_trees']
    n_features = results['X_train'].shape[1]


    ax1.plot(n_trees_list, results['oob_mse_rf_full'],
             label=f"Random Forest (all {n_features} features)",
             color="orange", linewidth=2)
    ax1.plot(n_trees_list, results['oob_mse_rf_half'],
             label=f"Random Forest (p/2 = {max(1, n_features // 2)} features)",
             color="skyblue", linewidth=2)
    ax1.plot(n_trees_list, results['oob_mse_rf_sqrt'],
             label=f"Random Forest (√p = {max(1, int(np.sqrt(n_features)))} features)",
             color="teal", linewidth=2)
    ax1.plot(n_trees_list, results['oob_mse_rf_log2'],
             label=f"Random Forest (log₂p = {max(1, int(np.log2(n_features)))} features)",
             color="purple", linewidth=2)
    ax1.plot(n_trees_list, results['oob_mse_bagging'],
             label="Bagging (no feature subsampling)",
             color="black", linestyle="--", linewidth=2)

    ax1.set_xlabel("Number of Trees")
    ax1.set_ylabel("Out-of-Bag MSE")
    ax1.set_title("Ensemble Methods Comparison\n(Out-of-Bag Performance)")
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # Training time plot
    ax2.plot(n_trees_list, results['training_times'],
             color="red", marker='o', linewidth=2, markersize=4)
    ax2.set_xlabel("Number of Trees")
    ax2.set_ylabel("Training Time (seconds)")
    ax2.set_title("Training Time vs Number of Trees")
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    # Print summary statistics
    print("\n"+"="*60)
    print("PERFORMANCE SUMMARY")
    print("="*60)

    methods = ['Bagging', 'RF (all features)', 'RF (p/2)', 'RF (√p)', 'RF (log₂p)']
    oob_results = [
        results['oob_mse_bagging'],
        results['oob_mse_rf_full'],
        results['oob_mse_rf_half'],
        results['oob_mse_rf_sqrt'],
        results['oob_mse_rf_log2']
    ]

    for method, mse_list in zip(methods, oob_results):
        # Filter out NaN values
        valid_mse = [mse for mse in mse_list if not np.isnan(mse)]
        if valid_mse:
            best_mse = min(valid_mse)
            best_idx = mse_list.index(best_mse)
            best_trees = n_trees_list[best_idx]
            final_mse = valid_mse[-1]
            print(f"{method:<20} | Best: {best_mse:.4f} @ {best_trees} trees | Final: {final_mse:.4f}")
        else:
            print(f"{method:<20} | No valid results")

    print("="*60)
    print(f"Total training time: {sum(results['training_times']):.2f} seconds")

# Example usage:
results = compare_ensemble_methods(X_train, y_train, max_trees=200, step_size=20)
plot_ensemble_comparison(results)

# 2. Train Random Forest with m = sqrt(p), 100 trees, and OOB enabled
rf_sqrt = RandomForestRegressor(
    n_estimators=100,
    max_features='sqrt',     # m = sqrt(p)
    bootstrap=True,
    oob_score=True,
    random_state=42,
    n_jobs=-1
)
rf_sqrt.fit(X_train, y_train)

# 3. Compute OOB R^2 and Test R^2
oob_r2 = rf_sqrt.oob_score_
y_pred_test = rf_sqrt.predict(X_test)
test_r2 = r2_score(y_test, y_pred_test)

# 4. Print results
print(f"✅ OOB R² score:  {oob_r2:.3f}")
print(f"✅ Test R² score: {test_r2:.3f}")

"""## grad boosting and svm"""

!pip install catboost

import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# CatBoost import with error handling
try:
    from catboost import CatBoostRegressor
    CATBOOST_AVAILABLE = True
except ImportError:
    print("CatBoost not installed. Install with: pip install catboost")
    CATBOOST_AVAILABLE = False

def simple_model_comparison(X, y, test_size=0.2, random_state=42):
    """
    Simple comparison of CatBoost and SVM with strong overfitting prevention.

    Parameters:
    -----------
    X : array-like, shape (n_samples, n_features)
        Feature matrix
    y : array-like, shape (n_samples,)
        Target values
    test_size : float, default=0.2
        Proportion of data for testing
    random_state : int, default=42
        Random state for reproducibility

    Returns:
    --------
    dict : Dictionary containing results and fitted models
    """

    print(f"Dataset: {X.shape[0]} samples, {X.shape[1]} features")
    print(f"Target range: [{y.min():.3f}, {y.max():.3f}]")

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )

    # Scale features for SVM
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    results = {}

    # =================================================================
    # CATBOOST - Conservative parameters to prevent overfitting
    # =================================================================
    if CATBOOST_AVAILABLE:
        print("\n🟠 Training CatBoost...")

        catboost_model = CatBoostRegressor(
            # Conservative settings to prevent overfitting
            iterations=100,              # Fewer iterations
            learning_rate=0.05,          # Lower learning rate
            depth=4,                     # Shallow trees
            l2_leaf_reg=10,              # Strong L2 regularization

            # Overfitting prevention
            early_stopping_rounds=20,    # Stop early if no improvement
            use_best_model=True,         # Use best model from validation
            eval_fraction=0.1,           # Use 10% for internal validation

            # Other settings
            random_state=random_state,
            verbose=False,
            bootstrap_type='Bayesian',   # More robust than default
            bagging_temperature=0.5      # Reduce overfitting in bagging
        )

        # Fit with validation set for early stopping
        catboost_model.fit(X_train, y_train)

        # Predictions and metrics
        y_pred_cat = catboost_model.predict(X_test)
        cat_mse = mean_squared_error(y_test, y_pred_cat)
        cat_r2 = r2_score(y_test, y_pred_cat)

        # Cross-validation for more robust evaluation
        cv_scores_cat = cross_val_score(
            catboost_model, X_train, y_train,
            cv=5, scoring='neg_mean_squared_error'
        )

        results['catboost'] = {
            'model': catboost_model,
            'test_mse': cat_mse,
            'test_r2': cat_r2,
            'cv_mse_mean': -cv_scores_cat.mean(),
            'cv_mse_std': cv_scores_cat.std(),
            'predictions': y_pred_cat
        }

        print(f"   Test MSE: {cat_mse:.4f}")
        print(f"   Test R²: {cat_r2:.4f}")
        print(f"   CV MSE: {-cv_scores_cat.mean():.4f} (±{cv_scores_cat.std():.4f})")

    # =================================================================
    # SVM - Conservative parameters to prevent overfitting
    # =================================================================
    print("\n🔵 Training SVM...")

    svm_model = SVR(
        # Conservative settings to prevent overfitting
        kernel='rbf',
        C=1.0,                    # Moderate regularization (lower C = more regularization)
        gamma='scale',            # Automatic gamma scaling
        epsilon=0.1,              # Tolerance for stopping criterion

        # Additional overfitting prevention
        shrinking=True,           # Use shrinking heuristic
        cache_size=200,           # Memory cache
        max_iter=1000            # Limit iterations
    )

    # Fit the model
    svm_model.fit(X_train_scaled, y_train)

    # Predictions and metrics
    y_pred_svm = svm_model.predict(X_test_scaled)
    svm_mse = mean_squared_error(y_test, y_pred_svm)
    svm_r2 = r2_score(y_test, y_pred_svm)

    # Cross-validation
    cv_scores_svm = cross_val_score(
        svm_model, X_train_scaled, y_train,
        cv=5, scoring='neg_mean_squared_error'
    )

    results['svm'] = {
        'model': svm_model,
        'test_mse': svm_mse,
        'test_r2': svm_r2,
        'cv_mse_mean': -cv_scores_svm.mean(),
        'cv_mse_std': cv_scores_svm.std(),
        'predictions': y_pred_svm
    }

    print(f"   Test MSE: {svm_mse:.4f}")
    print(f"   Test R²: {svm_r2:.4f}")
    print(f"   CV MSE: {-cv_scores_svm.mean():.4f} (±{cv_scores_svm.std():.4f})")

    # Store additional data
    results['test_data'] = {'X_test': X_test, 'y_test': y_test}
    results['scaler'] = scaler

    return results

def plot_simple_comparison(results):
    """Simple visualization of model comparison"""

    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    fig.suptitle('Model Comparison (Overfitting Prevention)', fontsize=14, fontweight='bold')

    models = [k for k in results.keys() if k not in ['test_data', 'scaler']]
    colors = {'catboost': 'orange', 'svm': 'blue'}

    # 1. Test MSE Comparison
    ax1 = axes[0]
    test_mses = [results[model]['test_mse'] for model in models]
    bars = ax1.bar(models, test_mses, color=[colors[m] for m in models], alpha=0.7)
    ax1.set_ylabel('Test MSE')
    ax1.set_title('Test Performance')
    ax1.grid(True, alpha=0.3)

    # Add values on bars
    for bar, mse in zip(bars, test_mses):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height,
                f'{mse:.3f}', ha='center', va='bottom')

    # 2. Cross-validation comparison with error bars
    ax2 = axes[1]
    cv_means = [results[model]['cv_mse_mean'] for model in models]
    cv_stds = [results[model]['cv_mse_std'] for model in models]

    bars = ax2.bar(models, cv_means, yerr=cv_stds,
                   color=[colors[m] for m in models], alpha=0.7, capsize=5)
    ax2.set_ylabel('Cross-Validation MSE')
    ax2.set_title('CV Performance (5-fold)')
    ax2.grid(True, alpha=0.3)

    # 3. Predictions vs Actual
    ax3 = axes[2]
    y_test = results['test_data']['y_test']

    for model in models:
        y_pred = results[model]['predictions']
        ax3.scatter(y_test, y_pred, alpha=0.6,
                   color=colors[model], label=model.capitalize(), s=40)

    # Perfect prediction line
    min_val = y_test.min()
    max_val = y_test.max()
    ax3.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8)

    ax3.set_xlabel('Actual Values')
    ax3.set_ylabel('Predicted Values')
    ax3.set_title('Predictions vs Actual')
    ax3.legend()
    ax3.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    # Print summary
    print("\n" + "="*50)
    print("SUMMARY (with overfitting prevention)")
    print("="*50)

    best_model = None
    best_mse = float('inf')

    for model in models:
        test_mse = results[model]['test_mse']
        cv_mse = results[model]['cv_mse_mean']
        cv_std = results[model]['cv_mse_std']
        r2 = results[model]['test_r2']

        print(f"\n{model.upper()}:")
        print(f"  Test MSE: {test_mse:.4f}")
        print(f"  Test R²:  {r2:.4f}")
        print(f"  CV MSE:   {cv_mse:.4f} (±{cv_std:.4f})")

        # Check for overfitting (large gap between CV and test performance)
        if abs(test_mse - cv_mse) > cv_std * 2:
            print(f"  ⚠️  Potential overfitting detected!")
        else:
            print(f"  ✅ Good generalization")

        if test_mse < best_mse:
            best_mse = test_mse
            best_model = model

    if best_model:
        print(f"\n🏆 Best Model: {best_model.upper()}")

    print("="*50)

def evaluate_overfitting(results):
    """Check for signs of overfitting"""
    print("\n🔍 OVERFITTING ANALYSIS")
    print("-" * 30)

    models = [k for k in results.keys() if k not in ['test_data', 'scaler']]

    for model in models:
        test_mse = results[model]['test_mse']
        cv_mse = results[model]['cv_mse_mean']
        cv_std = results[model]['cv_mse_std']

        # Calculate performance gap
        gap = abs(test_mse - cv_mse)
        gap_ratio = gap / cv_mse

        print(f"\n{model.upper()}:")
        print(f"  CV MSE:    {cv_mse:.4f}")
        print(f"  Test MSE:  {test_mse:.4f}")
        print(f"  Gap:       {gap:.4f} ({gap_ratio*100:.1f}%)")

        if gap > cv_std * 2:
            print(f"  Status:    ⚠️  HIGH OVERFITTING RISK")
        elif gap > cv_std:
            print(f"  Status:    ⚡ MODERATE OVERFITTING")
        else:
            print(f"  Status:    ✅ GOOD GENERALIZATION")

# Simple usage function
def compare_models(X, y, **kwargs):
    """One-line function to compare models with overfitting prevention"""
    results = simple_model_comparison(X, y, **kwargs)
    plot_simple_comparison(results)
    evaluate_overfitting(results)
    return results

# Example usage:
results = compare_models(X_train, y_train)
#
# # Access individual models:
catboost_model = results['catboost']['model']
svm_model = results['svm']['model']

catboost_model = CatBoostRegressor()
summary = catboost_model.select_features(X_train, y_train, features_for_select=X_train.columns, num_features_to_select=15, verbose=False)

print('Selected features indices:', summary['selected_features'])
print('Selected features names:', summary['selected_features_names'])
print('Eliminated features indices:', summary['eliminated_features'])
print('Eliminated features names:', summary['eliminated_features_names'])

# from catboost import CatBoostRegressor
# from sklearn.model_selection import GridSearchCV

# # Define parameter grid for search
# param_grid = {
#     'depth': [4, 6, 8],
#     'l2_leaf_reg': [1, 3, 5],
#     'random_state': [42]
# }

# # Build CatBoost regressor (important: silent mode!)
# catboost = CatBoostRegressor(verbose=False)

# # Use GridSearchCV for tuning
# grid_search = GridSearchCV(estimator=catboost,
#                            param_grid=param_grid,
#                            scoring='r2',
#                            cv=5,
#                            n_jobs=-1,  # parallelism
#                            verbose=1)

# # Fit only on selected features
# grid_search.fit(X_train[summary['selected_features_names']], y_train)

# # Best parameters found
# print("Best parameters:", grid_search.best_params_)
# print("Best R2 score (CV):", grid_search.best_score_)

# # Use best estimator to predict on test set
# catboost_best = grid_search.best_estimator_
# catboost_preds = catboost_best.predict(X_test[summary['selected_features_names']])
# catboost_r2 = r2_score(y_test, catboost_preds)
# print("Test R2 score:", catboost_r2)

from catboost import CatBoostRegressor


catboost = CatBoostRegressor(verbose=False, depth=4, l2_leaf_reg=20, random_state=42, subsample=0.6, colsample_bylevel=.6)
catboost.fit(X_train[summary['selected_features_names']], y_train)


y_train_cb = catboost.predict(X_train[summary['selected_features_names']])
y_test_cb = catboost.predict(X_test[summary['selected_features_names']])

print(f"train_r2 = {r2_score(y_train, y_train_cb)}, test_r2 = {r2_score(y_test, y_test_cb)}")
print(f"train_mse = {mean_squared_error(y_train, y_train_cb)}, test_mse = {mean_squared_error(y_test, y_test_cb)}")

import matplotlib.pyplot as plt

feature_importances = catboost.get_feature_importance()
features = X_train[summary['selected_features_names']].columns

feat_imp_df = pd.DataFrame({'feature': features, 'importance': feature_importances})
feat_imp_df = feat_imp_df.sort_values(by='importance', ascending=False)

plt.figure(figsize=(10,6))
plt.barh(feat_imp_df['feature'], feat_imp_df['importance'])
plt.gca().invert_yaxis()
plt.show()

catboost_model = results['catboost']['model']
svm_model = results['svm']['model']

"""## Final results"""

import pandas as pd
from sklearn.metrics import mean_squared_error, r2_score

# Prepare lists to store results
models = []
train_r2_list = []
train_mse_list = []
test_r2_list = []
test_mse_list = []

# --- Linear model (forward selection) ---
X_train_forward = sm.add_constant(X_train[final_betas.index.values])
X_test_forward = sm.add_constant(X_test[final_betas.index.values])

forward_train_preds = final_model.predict(X_train_forward)
forward_test_preds = final_model.predict(X_test_forward)

models.append('Forward Selection')
train_r2_list.append(r2_score(y_train, forward_train_preds))
train_mse_list.append(mean_squared_error(y_train, forward_train_preds))
test_r2_list.append(r2_score(y_test, forward_test_preds))
test_mse_list.append(mean_squared_error(y_test, forward_test_preds))

# --- Lasso ---
models.append('Lasso')
train_r2_list.append(lasso_cv_model.score(X_train, y_train))
train_mse_list.append(mean_squared_error(y_train, lasso_cv_model.predict(X_train)))
test_r2_list.append(lasso_cv_model.score(X_test, y_test))
test_mse_list.append(mean_squared_error(y_test, lasso_cv_model.predict(X_test)))

# --- Ridge ---
models.append('Ridge')
train_r2_list.append(ridge_cv_model.score(X_train, y_train))
train_mse_list.append(mean_squared_error(y_train, ridge_cv_model.predict(X_train)))
test_r2_list.append(ridge_cv_model.score(X_test, y_test))
test_mse_list.append(mean_squared_error(y_test, ridge_cv_model.predict(X_test)))

# --- Random Forest ---
y_train_rf = rf_sqrt.predict(X_train)
y_test_rf = rf_sqrt.predict(X_test)

models.append('Random Forest')
train_r2_list.append(r2_score(y_train, y_train_rf))
train_mse_list.append(mean_squared_error(y_train, y_train_rf))
test_r2_list.append(r2_score(y_test, y_test_rf))
test_mse_list.append(mean_squared_error(y_test, y_test_rf))

# --- SVM ---
y_train_svm = svm_model.predict(X_train)
y_test_svm = svm_model.predict(X_test)

models.append('SVM')
train_r2_list.append(r2_score(y_train, y_train_svm))
train_mse_list.append(mean_squared_error(y_train, y_train_svm))
test_r2_list.append(r2_score(y_test, y_test_svm))
test_mse_list.append(mean_squared_error(y_test, y_test_svm))

# --- CatBoost ---
y_train_cb = catboost.predict(X_train[summary['selected_features_names']])
y_test_cb = catboost.predict(X_test[summary['selected_features_names']])

models.append('CatBoost')
train_r2_list.append(r2_score(y_train, y_train_cb))
train_mse_list.append(mean_squared_error(y_train, y_train_cb))
test_r2_list.append(r2_score(y_test, y_test_cb))
test_mse_list.append(mean_squared_error(y_test, y_test_cb))

# --- Build final report ---
report = pd.DataFrame({
    'Model': models,
    'Train R²': train_r2_list,
    'Train MSE': train_mse_list,
    'Test R²': test_r2_list,
    'Test MSE': test_mse_list,
})

# Pretty print
print(report.to_string(index=False))

report # without logged

report # with logged

"""#Classification Task"""

df.shape

df.columns

plt.hist(df['logged_max_of_timestamp'], bins = 40)

median_timestamp = df['logged_max_of_timestamp'].median()

df['Class'] = (df['logged_max_of_timestamp'] > median_timestamp).astype(int)

counts = df['Class'].value_counts()
counts

df.groupby('Class').mean().T.head(10)  # Transposed for easier reading

correlations = df.drop(columns=['max_of_timestamp', 'logged_max_of_timestamp']).corr()['Class'].sort_values(ascending=False)

top_features = correlations[correlations.index != 'Class'].abs().sort_values(ascending=False).head(10).index

# Plot configuration
n_features = len(top_features)
n_cols = 3
n_rows = int(np.ceil(n_features / n_cols))

fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))
axes = axes.flatten()

# Plot each boxplot
for i, feature in enumerate(top_features):
    sns.boxplot(x='Class', y=feature, data=df, ax=axes[i])
    axes[i].set_title(f"Boxplot of {feature} by Class")

# Remove unused subplots
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

sns.pairplot(df, vars=top_features, hue='Class')

feature = top_features[0]

sns.histplot(data=df, x=feature, hue='Class', kde=True, bins=20, element="step", stat="density")
plt.title(f"Distribution of {feature} by Class")
plt.show()

# Define features to plot (all numeric features excluding ID/target)
features = df.drop(columns=['sum_of_timestamp', 'Class', 'episode_index'], errors='ignore').select_dtypes(include=np.number).columns

# Subplot configuration
n_features = len(features)
n_cols = 3
n_rows = int(np.ceil(n_features / n_cols))
fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))
axes = axes.flatten()  # Flatten to 1D array for easy indexing

# Plot each feature
for i, feature in enumerate(features):
    sns.histplot(data=df, x=feature, hue='Class', kde=True, bins=30,
                 element="step", stat="density", ax=axes[i])
    axes[i].set_title(f"Distribution of {feature} by Class")

# Remove any unused subplots
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

y = df['Class']
X = df.drop(columns=['max_of_timestamp', 'Class', 'episode_index', 'logged_max_of_timestamp'], errors='ignore')

X.shape

feature_names = X.columns.tolist()
feature_names

from sklearn.ensemble import BaggingClassifier, RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Bagging with Decision Tree
bag = BaggingClassifier(estimator=DecisionTreeClassifier(),
                        n_estimators=100,
                        bootstrap=True,
                        oob_score=True,
                        random_state=42)
bag.fit(X_train, y_train)
y_pred_bag = bag.predict(X_test)

# Random Forest
rf = RandomForestClassifier(n_estimators=100, bootstrap=True, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

from sklearn.svm import SVC

# Train Linear SVM
svm_linear = SVC(kernel='linear', random_state=42)
svm_linear.fit(X, y)
y_pred_svm = svm_linear.predict(X_test)

# Extract weights and bias
w = svm_linear.coef_[0]
b = svm_linear.intercept_[0]
feature_names = X.columns.tolist()

# Print the linear decision function
terms = [f"{w[i]:+0.3f}·x{i+1}" for i in range(len(w))]
equation = " + ".join(terms) + f" {b:+.3f}"
print("\nLinear SVM Decision Function:")
print(f"f(x) = {equation}")

# Feature explanation table
print("\n🧾 Feature Explanation Table:")
print(f"{'xᵢ':<5} {'Feature Name':<40} {'Weight':>10}")
print("-" * 60)
for i, (name, coef) in enumerate(zip(feature_names, w)):
    print(f"x{i+1:<3} {name:<40} {coef:>+10.4f}")

from sklearn.ensemble import GradientBoostingClassifier


# Gradient Boosting
gb = GradientBoostingClassifier(n_estimators=100, # Number of boosting stages (trees)
                                learning_rate=0.1, # Step size shrinkage
                                max_depth=3, # Maximum depth of individual trees
                                random_state=42)
gb.fit(X_train, y_train)
y_pred_gb = gb.predict(X_test)

# Print results
print(f"Accuracy - Bagging: {accuracy_score(y_test, y_pred_bag):.2%}")
print(f"OOB Score - Bagging: {bag.oob_score_:.2%}")
print(f"Accuracy - Random Forest: {accuracy_score(y_test, y_pred_rf):.2%}")
print(f"Accuracy - SVM: {accuracy_score(y_test, y_pred_svm):.2%}")
print(f"Gradient Boosting Accuracy: {accuracy_score(y_test, y_pred_gb):.2%}")

import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay, confusion_matrix

# Plot confusion matrices in a 2x2 grid
fig, axes = plt.subplots(2, 2, figsize=(12, 10)) # Changed to 2x2 subplots with adjusted figsize
fig.suptitle("Confusion Matrices for Classification Models", fontsize=16) # Increased font size for title

# Bagging (Top-left)
ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_bag), display_labels=[0, 1]).plot(ax=axes[0, 0], values_format='d')
axes[0, 0].set_title("Bagging")

# Random Forest (Top-right)
ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_rf), display_labels=[0, 1]).plot(ax=axes[0, 1], values_format='d')
axes[0, 1].set_title("Random Forest")

# SVM (Bottom-left)
ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_svm), display_labels=[0, 1]).plot(ax=axes[1, 0], values_format='d')
axes[1, 0].set_title("SVM")

# Gradient Boosting (Bottom-right)
ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_gb), display_labels=[0, 1]).plot(ax=axes[1, 1], values_format='d')
axes[1, 1].set_title("Gradient Boosting")

plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust rect to prevent title overlap
plt.show()

metrics = {}

metrics['Bagging'] = {
    'Accuracy': accuracy_score(y_test, y_pred_bag),
    'Precision': precision_score(y_test, y_pred_bag),
    'Recall': recall_score(y_test, y_pred_bag),
    'F1-Score': f1_score(y_test, y_pred_bag)
}

metrics['Random Forest'] = {
    'Accuracy': accuracy_score(y_test, y_pred_rf),
    'Precision': precision_score(y_test, y_pred_rf),
    'Recall': recall_score(y_test, y_pred_rf),
    'F1-Score': f1_score(y_test, y_pred_rf)
}

metrics['SVM'] = {
    'Accuracy': accuracy_score(y_test, y_pred_svm),
    'Precision': precision_score(y_test, y_pred_svm),
    'Recall': recall_score(y_test, y_pred_svm),
    'F1-Score': f1_score(y_test, y_pred_svm)
}

metrics['Gradient Boosting'] = {
    'Accuracy': accuracy_score(y_test, y_pred_gb),
    'Precision': precision_score(y_test, y_pred_gb),
    'Recall': recall_score(y_test, y_pred_gb),
    'F1-Score': f1_score(y_test, y_pred_gb)
}

metrics_df = pd.DataFrame.from_dict(metrics, orient='index')

print("\n--- Classification Model Performance Metrics ---")
print(metrics_df.round(4))

from sklearn.svm import SVC
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_score, StratifiedKFold
import numpy as np


# Define SVM kernels to test
kernels = ['linear', 'rbf', 'poly', 'sigmoid']

# Setup cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Evaluate each kernel
for kernel in kernels:
    print(f"\n🔍 SVM Kernel: {kernel.upper()}")

    # Initialize model
    if kernel == 'poly':
        model = SVC(kernel=kernel, degree=3)
    else:
        model = SVC(kernel=kernel)

    # Cross-validated accuracy
    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')
    print(f"→ Accuracy scores: {np.round(scores, 4)}")
    print(f"→ Mean accuracy: {scores.mean():.4f}")
    print(f"→ Std deviation: {scores.std():.4f}")

    # 🔁 Refit on full data to inspect hyperplane / support vectors
    model.fit(X, y)

    if kernel == 'linear':
        w = model.coef_[0]
        b = model.intercept_[0]
        print(f"→ Linear hyperplane equation:")
        terms = [f"{w[i]:+.3f}·x{i+1}" for i in range(len(w))]
        equation = " + ".join(terms) + f" {b:+.3f}"
        print(f"f(x) = {equation}")

    else:
        sv = model.support_vectors_
        dual_coefs = model.dual_coef_[0]
        intercept = model.intercept_[0]
        n_sv = len(sv)

        print(f"→ Number of support vectors: {n_sv}")
        print(f"→ Intercept (b): {intercept:.3f}")
        print("→ Decision function structure:")

        if kernel == 'rbf':
            print(r"f(x) = Σ αᵢ·yᵢ·exp(-γ·||xᵢ - x||²) + b")
        elif kernel == 'poly':
            print(r"f(x) = Σ αᵢ·yᵢ·(γ·xᵢᵗ·x + coef0)^degree + b")
        elif kernel == 'sigmoid':
            print(r"f(x) = Σ αᵢ·yᵢ·tanh(γ·xᵢᵗ·x + coef0) + b")
        print("  (evaluated using support vectors)")

from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score, StratifiedKFold
import numpy as np

# Use your real dataset
y = df['Class']
X = df.drop(columns=['max_of_timestamp', 'Class', 'episode_index', 'logged_max_of_timestamp'], errors='ignore')
feature_names = X.columns.tolist()

# Define SVM kernels to test
kernels = ['linear', 'rbf', 'poly', 'sigmoid']

# Setup cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Evaluate each kernel
for kernel in kernels:
    print(f"\n🔍 SVM Kernel: {kernel.upper()}")

    # Initialize model
    if kernel == 'poly':
        model = SVC(kernel=kernel, degree=3)
    else:
        model = SVC(kernel=kernel)

    # Cross-validated accuracy
    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')
    print(f"→ Accuracy scores: {np.round(scores, 4)}")
    print(f"→ Mean accuracy: {scores.mean():.4f}")
    print(f"→ Std deviation: {scores.std():.4f}")

    # 🔁 Refit on full data to extract interpretation info
    model.fit(X, y)

    if kernel == 'linear':
        # Linear hyperplane: wᵀx + b
        w = model.coef_[0]
        b = model.intercept_[0]
        print(f"\n→ Linear hyperplane equation:")
        terms = [f"{w[i]:+.3f}·x{i+1}" for i in range(len(w))]
        equation = " + ".join(terms) + f" {b:+.3f}"
        print(f"f(x) = {equation}")

        # Print feature explanation table
        print("\n🧾 Feature Explanation Table:")
        print(f"{'xᵢ':<5} {'Feature Name':<40}")
        print("-" * 50)
        for i, name in enumerate(feature_names):
            print(f"x{i+1:<3} {name:<40}")

    else:
        # Nonlinear SVM: kernel-based decision function
        sv = model.support_vectors_
        dual_coefs = model.dual_coef_[0]
        intercept = model.intercept_[0]
        n_sv = len(sv)

        print(f"→ Number of support vectors: {n_sv}")
        print(f"→ Intercept (b): {intercept:.3f}")
        print("→ Decision function structure:")

        if kernel == 'rbf':
            print(r"f(x) = Σ αᵢ·yᵢ·exp(-γ·||xᵢ - x||²) + b")
        elif kernel == 'poly':
            print(r"f(x) = Σ αᵢ·yᵢ·(γ·xᵢᵗ·x + coef0)^degree + b")
        elif kernel == 'sigmoid':
            print(r"f(x) = Σ αᵢ·yᵢ·tanh(γ·xᵢᵗ·x + coef0) + b")
        print("  (evaluated using support vectors)")

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import cross_val_score, StratifiedKFold
import numpy as np
import pandas as pd # Import pandas if df is not globally available

# Use your real dataset
# Ensure 'Class' is your target variable. If it's 'is_above_median', adjust here.
X = df.drop(columns=['max_of_timestamp', 'Class', 'episode_index', 'logged_max_of_timestamp'], errors='ignore')
y = df['Class']
feature_names = X.columns.tolist()


# Setup cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# --- Gradient Boosting Classifier ---
print("\n=== Gradient Boosting Classifier ===")

# Initialize the Gradient Boosting model
# Common hyperparameters to start with:
# n_estimators: number of boosting stages (trees)
# learning_rate: shrinks the contribution of each tree
# max_depth: maximum depth of the individual regression estimators
model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

# Cross-validated accuracy
scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')
print(f"\n→ Accuracy scores (Cross-Validation): {np.round(scores, 4)}")
print(f"→ Mean accuracy: {scores.mean():.4f}")
print(f"→ Std deviation: {scores.std():.4f}")

# 🔁 Refit on full data to extract interpretation info (e.g., Feature Importances)
model.fit(X, y)

# --- Interpretability for Gradient Boosting ---
# Gradient Boosting models provide feature importances, which indicate the
# relative importance of each feature in predicting the target variable.
feature_importances = model.feature_importances_

# Create a DataFrame for better display
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': feature_importances
})

# Sort by importance
importance_df = importance_df.sort_values(by='Importance', ascending=False)

print("\n📊 Feature Importances (Higher is more important):")
print(importance_df)

# You can also visualize these importances, for example:
# import matplotlib.pyplot as plt
# import seaborn as sns
# plt.figure(figsize=(10, 6))
# sns.barplot(x='Importance', y='Feature', data=importance_df)
# plt.title('Feature Importances for Gradient Boosting Classifier')
# plt.tight_layout()
# plt.show()

"""TODO List:

- check a code of the other group
- SVM
- gradient boosting
- log(target)
- write a report (PDF) and upload it
- collect together classification and regression task.
- make it more pretty
- maybe add some analysis and features (if have ideas)
-
"""